---
layout: article
title: Understanding Principal Component Analysis (PCA) 
key: 20180922
tags:
- data visualization
- machine learning
mathjax: true
---
During machine learning practice or data analysis, when we get a new dataset, what we can do first is to visualize the data to observe how the data distribute and what the relationship between datapoints. We can easily visualize dataset with 2 or 3 variables by plotting 2-demension(2D) or 3-dimension(3D) figure. However, most of the datasets have quite a large number of variables. Therefore it is important to get to know how to visulize high-dimension dataset. 

<!--more-->

I would like to discuss two popular techniques we usually use for high-dimension dataset visualization: Principal Component Analysis **(PCA)** and T-distributed Stochastic Neighbor Embedding **(t-SNE)**. We will focus on PCA on this post and more about t-SNE in later post.

## What is PCA

PCA tries to out a smaller set of new variables(principal components, PCs) which can capture most of the variation in dataset. Intuitively, a dimension that has more variability can explain more about the happenings. We can get some simple examples to make it easy to understand. 
![pca_1](https://raw.githubusercontent.com/xiaoyanzhuo/xiaoyanzhuo.github.io/master/_posts/figures/pca_1.png){:width="270px" height="150px"}
![pca_2](https://raw.githubusercontent.com/xiaoyanzhuo/xiaoyanzhuo.github.io/master/_posts/figures/pca_2.png){:width="270px" height="150px"}


~~*to be continued*~~

## How PCA works
1. normalization
2. best fitting (projection)
3. egienvalue, eigenvector
4. loading score, screen plot

## Examples









